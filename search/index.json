[{"content":"对于p人，一旦没有强烈的外在ddl驱动，对时间的掌控就只能凭一时兴起。为了养成好的习惯，加上想彻底改变不健康的作息，是时候提高执行力了。\n在开始之前，我先花了一个晚上梳理我到底希望怎么样的日程管理。\n复盘过去的日程管理 读研时候一直在用notion作为笔记数据库，也用得很顺手。我有创建一个这样的主页来管理所有实验室相关的事情。\n上面的几个版块分别是：\n实验室SOP：韩国实验室里规矩很多，初来乍到的我就把那些前辈们的“教导”一一记下来，免得之后犯错，也包括一些密码之类的信息） Course：选课相关的，课件、作业、笔记、考试等等 Seminar：实验室里的论文研讨还有其他教授的讲座等记录 Lab Research：项目还有论文的内容 Topics：看论文的时候发现的点子，一些小的笔记 因为主要活动都在实验室，所以就在下面创建了一个database记录实验室的任务：有要做的事情就先存到Task Inbox，设置了一项计算ddl在本周内的项目来提醒自己，其他两个视图分别展示seminar和组会的日程。因为notion是每项都可以展开成单独的页面，所以很方便记录相关的内容。\n在两年的磨合里，实践证明这样安排很适合我，但是现在回国之后这个template显然不够了。因为基本是以“实验室的任务”为导向的一种划分方式，我还希望能把生活上的种种琐碎时间也利用起来。\n明确我的需求 如果分类一天里面要做的事，一类是是和时间强关联的，比如十点半开会，另一类是动态可变的，比如去寄快递这种优先级不高且允许调整的。原来尝试过按照小时把一天进行分割，但是效果并不好，因为可能会有突发的外来任务打乱你的安排，这也是为什么很早就抛弃纸质的原因，电子化的优点就是随时增删查改。比起列表化的日程，我更喜欢日历视图，一周、一个月甚至更长时间的任务都能展开在眼前，对于持续时间长的任务也更友好。当然，我也考虑过直接在日历下面安排日程，但是发现不能替代，因为使用日历就默认所有任务属于分类一，分类二的任务会难以放置。另外，由于主要设备是iPhone、iPad和MacBook，而且大概率之后会购入iWatch，所以数据能及时在各个平台查看也是需求之一。\n总结起来就是快捷、日历化、全平台。\nnotion一直用的是电脑版，app版的体验并不好，前面的“列表每一项都是单独页面“反而变成了缺点，而且为“寄快递”这种事单独一条笔记对于日程管理来说”太多了“。广受好评的滴答清单、极简待办、番茄todo、小日常我都用过，但是最后都卸载了。原因是用起来不顺手，每次都要打开各种按钮分类设置很耗时。\n兜兜转转，最后决定使用ios原生的提醒事项（笑）。优点很明显：免费，操作简单，苹果全家桶的安全感，添加事项可以喊出siri帮忙，能直接通过拖拽关联其他app。\n所以最后，日常生活部分使用提醒事项，个人成长也就是学习工作继续用notion。\n开始安排 *提醒事项\n我首先搜索了一些博主是如何使用提醒事项的，比如下面这个视频用到GTD的方法来创建清单： 清单的设置其实思路和前面我给notion的设置如出一辙，都是先有一个总体的日程，再根据需要去细分。 我最后使用了下面五个列表：\n收集箱：存放所有的待办事项，无分类。 日常检查：重复性的任务。 执行清单：自主需要完成的任务。设置优先级，固定时间的任务拖到日历里面。 等待清单：不能完全自主，要依赖其他人安排的任务。 可能清单：可做可不做的任务。 原视频里还有项目清单，用于存放长期的项目，但是我觉得这类使用notion或者备忘录都更合适，比如计划旅游和添置设备，都需要找很多图文链接参考，可以直接作为一条笔记。\nnotion\nnotion单独开了一个日历页面记录每天的to do list，然后可以通过relation链接到对应的笔记database（比如我这里直接对应到力扣，这样就方便之后查看）\n使用报告 大概按照这样的搭配工作了一个月，notion用起来没什么很大区别，提醒事项倒是调整了一些。原本有设置很多定时的事情，比如每隔一个小时喝水，但是实际上触发频率有点高了，一做起事来会很讨厌被打断，所以只留下必须的“休息眼睛”，每个两个小时提醒一次。另外最好用的果然还是siri，比手动安排快太多。\n","date":"2023-07-15T22:52:50+08:00","image":"https://kkpetra.github.io/p/%E6%89%BE%E5%88%B0%E6%9C%80%E9%80%82%E5%90%88%E7%9A%84%E6%97%A5%E7%A8%8B%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/tixing-page_hu1ce3f046530b1d52f7a0c0fdd6ef3ba4_440174_120x120_fill_box_smart1_3.png","permalink":"https://kkpetra.github.io/p/%E6%89%BE%E5%88%B0%E6%9C%80%E9%80%82%E5%90%88%E7%9A%84%E6%97%A5%E7%A8%8B%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/","title":"找到最适合的日程管理工具"},{"content":"这篇主要是汇总了一些PyTorch实操中常见的函数。参数写法以及代码示例来自官方文档或者开源模型代码，图片出处见水印。\nPyTorch基本张量操作 创建tensor\n直接创建：torch.tensor(data) 从numpy创建：torch.from_numpy(np_array) 拼接tensor： torch.cat(tensors, dim=0, *, out=None)\n将张量按照 dim 维度进行拼接\n1 2 3 4 5 x = torch.randn(2, 3) # shape:torch.Size([4, 3]) torch.cat((x, x, x), 0) # shape:torch.Size([2, 6]) torch.cat((x, x, x), 1) 复制tensor：torch.tensor.repeat(size)\n参数表示在对应dim上的重复次数\n1 2 3 4 5 6 7 8 9 x = torch.tensor([1, 2, 3]) # 行重复4，列重复2 x.repeat(4, 2) \u0026#39;\u0026#39;\u0026#39; tensor([[ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3], [ 1, 2, 3, 1, 2, 3]]) \u0026#39;\u0026#39;\u0026#39; 改变维度\n二维矩阵重新排列：torch.transpose(input, dim0, dim1) 多维矩阵重新排列：torch.permute(input, dims) （与view()/reshape()的区别：permute()得到是转置，而非张开再重新排列，permute()更适合来处理高维）\n压缩去掉维数为1的的维度：torch.squeeze(input, dim=None, out=None) 给指定位置加上维数为一的维度：torch.unsqueeze(input, dim) 1 2 3 4 5 # 组合使用来处理图片数据格式 -\u0026gt; [batch_size, n_channels, hight, width] img_tensor = torch.from_numpy(img).float().permute(2, 0, 1).unsqueeze(0) # 读取的图片是numpy格式(h,w,c)但是torch处理的数据是(c,h,w) # 图片size比如是（28，28，3），那么 permute(2,0,1) 得到一个size为（3，28，28）的tensor # unsqueeze(0)后就会在0的位置加了一维，用于batchsize即批处理大小维度 PyTorch梯度计算 确认求导：requires_grad=True 自动求导：torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) 1 2 3 4 5 6 7 8 9 10 x = torch.tensor([3.], requires_grad=True) y = torch.pow(x, 2) # y = x**2 # 如果需要求 2 阶导，需要设置 create_graph=True，让一阶导数 grad_1 也拥有计算图 grad_1 = torch.autograd.grad(y, x, create_graph=True) # grad_1 = dy/dx = 2x = 2 * 3 = 6 print(grad_1) # 这里求 2 阶导 grad_2 = torch.autograd.grad(grad_1[0], x) # grad_2 = d(dy/dx)/dx = d(2x)/dx = 2 print(grad_2) 得到不具备梯度的张量：tensor.detach()\n训练网络的时候可能希望保持一部分的网络参数不变，只对其中一部分的参数进行调整；或者只训练部分分支网络，并不让其梯度对主网络的梯度造成影响，这时候就需要使用detach()函数来切断一些分支的反向传播。\n梯度清零：grad.zero_() 和 optimizer.zero_grad()\n每次反向传播求导时，计算的梯度不会自动清零。如果进行多次迭代计算梯度而没有清零，那么梯度会在前一次的基础上叠加。\n停止梯度计算：@torch.no_grad()\n在某些情况下并不需要对模型进行梯度更新，例如在模型推理时或者在评估模型性能时，可以将这句放在代码段前，关闭梯度计算（不会影响计算图的构建和前向传播的计算）。\nPyTorch数据处理 torch.utils.data Dataset和DataLoader\nDataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。而DataLoader定义了按batch加载数据集的方法，它是一个实现了__iter__方法的可迭代对象，每次迭代输出一个batch的数据。DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。\n在绝大部分情况下，用户只需实现Dataset的__len__方法和__getitem__方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Dataset(object): def __init__(self): pass def __len__(self): raise NotImplementedError def __getitem__(self,index): raise NotImplementedError class DataLoader(object): def __init__(self,dataset,batch_size,collate_fn,shuffle = True,drop_last = False): self.dataset = dataset self.sampler =torch.utils.data.RandomSampler if shuffle else \\ torch.utils.data.SequentialSampler self.batch_sampler = torch.utils.data.BatchSampler self.sample_iter = self.batch_sampler( self.sampler(range(len(dataset))), batch_size = batch_size,drop_last = drop_last) def __next__(self): indices = next(self.sample_iter) batch = self.collate_fn([self.dataset[i] for i in indices]) return batch PyTorch模型操作 torch.nn 模型自定义操作基本上都是继承 torch.nn.Module 类来实现的。\n模型参数： torch.nn.parameter.Parameter(data=None, requires_grad=True)\n模型容器\ntorch.nn.ParameterList：像 list 一样存储一堆参数。 torch.nn.ParameterDict：像 dict 一样存储一堆参数。 1 2 3 4 5 6 7 8 9 10 11 class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.params = nn.ParameterDict({ \u0026#39;left\u0026#39;: nn.Parameter(torch.randn(5, 10)), \u0026#39;right\u0026#39;: nn.Parameter(torch.randn(5, 10)) }) def forward(self, x, choice): x = self.params[choice].mm(x) return x torch.nn.Sequetial：按照顺序包装多个网络层，内部forward已经实现。 1 2 3 4 5 6 7 8 9 class single_conv(nn.Module): def __init__(self, in_ch, out_ch): super(single_conv, self).__init__() self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),) def forward(self, x): return self.conv(x) torch.nn.ModuleList：像 list 一样包装多个网络层，可以迭代。ModuleList中元素的先后顺序并不代表其在网络中的真实位置顺序，需要经过forward函数指定。 1 2 3 4 5 6 7 8 9 class ModuleList(nn.Module): def __init__(self): super(ModuleList, self).__init__() self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(20)]) def forward(self, x): for i, linear in enumerate(self.linears): x = linear(x) return x nn.ModuleDict：像 dict一样包装多个网络层，通过 (key, value) 的方式为每个网络层指定名称。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 创建了两个 ModuleDict，在前向传播时通过传入对应的 key 来执行对应的网络层 class ModuleDict(nn.Module): def __init__(self): super(ModuleDict, self).__init__() self.choices = nn.ModuleDict({ \u0026#39;conv\u0026#39;: nn.Conv2d(10, 10, 3), \u0026#39;pool\u0026#39;: nn.MaxPool2d(3) }) self.activations = nn.ModuleDict({ \u0026#39;relu\u0026#39;: nn.ReLU(), \u0026#39;prelu\u0026#39;: nn.PReLU() }) def forward(self, x, choice, act): x = self.choices[choice](x) x = self.activations[act](x) return x 创建模型过程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Module(nn.Module): def __init__(self): super(Module, self).__init__() # ...... def forward(self, x): # ...... return x data = ..... # 输入数据 # 实例化上面的类来定义模型 model = Module() # 前向传播 model(data) 模型保存：torch.save(model, path)\n模型读取：model = torch.load(path)\nDropout抑制模型过拟合：torch.nn.Dropout(p=0.5, inplace=False)\n模型训练：model.train()\n保证 BN 层能够用到每一批数据的均值和方差，对于 Dropout是随机取一部分网络连接来训练更新参数。\n模型评估：model.eval()\n模型测试前使用，不启用 Batch Normalization 和 Dropout，保证 BN 层能够用到所有数据的均值和方，对于 Dropout是不随机舍弃神经元，用所有网络连接。\nPyTorch常用vision功能函数 torch.nn.functional 差值计算：torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None, antialias=False) 1 img_tensor = torch.nn.functional.interpolate(img_tensor, [128, 128]) 扩充张量：torch.nn.functional.pad(input, pad, mode='constant', value=0) 1 2 3 4 5 6 7 8 9 10 11 12 # 当 pad 只有两个参数时，仅改变最后一个维度 # 当 pad 有四个参数，代表对最后两个维度扩充，pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数) # 当 pad 有六个参数时，代表对最后三个维度扩充，pad = (左边填充数， 右边填充数， 上边填充数， 下边填充数， 前边填充数，后边填充数) input = torch.randn([2,3,4,5]) padding = ( 1,2, 2,3, 3,4 ) print(input.shape) output = F.pad(output, padding) print(output.shape) # torch.Size([2, 10, 9, 8]) 把维度(B, Crr, H, w) 重新排列成 (B, C, Hr, wr)：torch.nn.functional.pixel_shuffle(*input*, *upscale_factor*)** 1 2 3 input = torch.randn(1, 9, 4, 4) output = torch.nn.functional.pixel_shuffle(input, 3) print(output.size()) # torch.Size([1, 1, 12, 12]) 从批量输入张量中提取滑动局部块：torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1) 1 unfolded_x = torch.nn.functional.unfold(x, block_size, stride=block_size) 将input通过grid映射到output上：grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None) 1 2 3 4 # input: [B, C, H_in, W_in] # grid: [B, H_out, W_out, 2] # output: [B, C, H_out, W_out] samples = torch.nn.functional.grid_sample(images, flow_grid, align_corners=True) PyTorch模型GPU训练 获取设备 1 device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) CPU和GPU的相互转换 1 2 3 4 5 6 7 8 9 # 从 CPU 到 GPU device = torch.device(\u0026#34;cuda\u0026#34;) tensor = tensor.to(device) module.to(device) # 从 GPU 到 CPU device = torch.device(cpu) tensor = tensor.to(\u0026#34;cpu\u0026#34;) module.to(\u0026#34;cpu\u0026#34;) 多GPU分布式训练：torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) 1 2 3 4 5 # 设置 2 个可见 GPU gpu_list = [0,1] gpu_list_str = \u0026#39;,\u0026#39;.join(map(str, gpu_list)) os.environ.setdefault(\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;, gpu_list_str) # 这里注意，需要指定一个 GPU 作为主 GPU PyTorch可视化 torch.utils.tensorboard 指定文件夹保存数据： SummaryWriter() 1 2 3 import torch from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() 可视化标量参数（损失函数、准确率等）： add_scalar(tag, scalar_value, global_step=None, walltime=None) 1 writer.add_scalar(\u0026#34;Loss/train\u0026#34;, loss, epoch) 添加图像数据：add_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW') 1 2 #tag (string) – Data identifier writer.add_image(k, img_dict[k], step_count) 终端打开命令：tensorboard --logdir=runs/ 参考链接 https://pytorch.org/docs/stable/index.html https://pytorch.apachecn.org/ https://pytorch.zhangxiann.com/ https://datawhalechina.github.io/thorough-pytorch/index.html https://www.cnblogs.com/qftie/p/16324068.html ","date":"2021-10-12T22:52:50+08:00","image":"https://kkpetra.github.io/p/pytorch%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/pytorch_hu65e3f337eb39eaddb85c63f1e4cc3b53_66072_120x120_fill_box_smart1_3.png","permalink":"https://kkpetra.github.io/p/pytorch%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"PyTorch基础学习记录"},{"content":"最近在准备实验室的论文研讨，自己的presentation slides返工几次，每次看都发现新的问题。把自己的材料跟其他同学的对比（韩国人真的很会做PPT），也向不少前辈请教了经验，下面是一些心得（反省）：\n在一开始，需要快读让别人明白这篇论文做了什么。 论文作者才是最懂的，跟着论文的思路来讲故事，自己的“意识流解读”不一定其他人能明白。 不要贪心，讲明白已有的内容，比为了堆页数而堆内容更好。 来龙去脉和主要贡献要说清楚： 输入什么、输出什么，以什么样的形态和内容出现。 某一个模块在这一部分的作用和使用理由。 略讲可以被替换的（通用的）部分，精讲不可被替换的（独创的）部分。 公式、图片等的标注要清晰，参数符号要统一格式，哪怕是求和公式上一个字母也可能产生迷惑。 对着讲稿念是万万不可的，要配合画图之类的手部动作。 演示文稿完成后，以“第一次听的人”的视角过一遍，可以发现很多隐藏的问题。 ","date":"2021-10-01T22:04:01+09:00","image":"https://kkpetra.github.io/p/%E5%A6%82%E4%BD%95%E6%9B%B4%E5%A5%BD%E5%9C%B0%E5%AE%8C%E6%88%90presentation/macbook_desk_hu2599a0d1ae07ad8d9df3c15004518743_1622462_120x120_fill_q75_box_smart1.jpg","permalink":"https://kkpetra.github.io/p/%E5%A6%82%E4%BD%95%E6%9B%B4%E5%A5%BD%E5%9C%B0%E5%AE%8C%E6%88%90presentation/","title":"如何更好地完成presentation"},{"content":"大概三年前搭建过个人博客，写了点零碎的文章，后面一直处于荒废状态。现在换了新的MacBook想折腾一下，于是start from scratch，从头开始建博客。这次使用hugo加Github Page的划算组合，因为有经验所以框架很快弄完，大部分时间都用来“装修”了。搜索了不少博主的装修心得，最后选择主题是stack（感谢作者！）。\n基建部分 两个仓库：存储源码的源仓库和一个用于github page网页部署的仓库。每次更新文章并运行hugo，生成的静态网页内容会存到public文件夹，只需要将public关联到github page仓库即可完成发布。\n具体过程可看如何用 GitHub Pages + Hugo 搭建个人博客\n主题修改 我需要的功能主题基本都提供了，只是根据喜好做了点小的修改。基本方法是在浏览器检查不满意的地方，找到对应的类然后去修改代码。\n更换字体 修改正文字体（layouts/partials/head/custom.html）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;style\u0026gt; /* Overwrite CSS variable */ :root { --article-font-family: \u0026#39;Noto Serif HK\u0026#39;, var(--base-font-family); } \u0026lt;/style\u0026gt; \u0026lt;script\u0026gt; ;(function () { const customFont = document.createElement(\u0026#39;link\u0026#39;) customFont.href = \u0026#39;https://fonts.googleapis.com/css2?family=Noto+Serif+HK:wght@300;400;500;600;700\u0026amp;display=swap\u0026#39; customFont.type = \u0026#39;text/css\u0026#39; customFont.rel = \u0026#39;stylesheet\u0026#39; document.head.appendChild(customFont) })() \u0026lt;/script\u0026gt; 修改全站字体（assets/scss/variables.scss）\n1 2 3 4 5 6 7 8 9 10 11 /** * Global font family */ :root { --sys-font-family: \u0026#39;Noto Serif HK\u0026#39;, -apple-system, BlinkMacSystemFont, \u0026#39;Segoe UI\u0026#39;, \u0026#39;Droid Sans\u0026#39;, \u0026#39;Helvetica Neue\u0026#39;; --zh-font-family: \u0026#39;Noto Serif HK\u0026#39;, \u0026#39;Hiragino Sans GB\u0026#39;, \u0026#39;Droid Sans Fallback\u0026#39;, \u0026#39;Microsoft YaHei\u0026#39;; --base-font-family: \u0026#39;Noto Serif HK\u0026#39;, var(--sys-font-family); --code-font-family: Menlo, Monaco, Consolas, \u0026#34;Courier New\u0026#34;, monospace; } 解决标点出现在中文字符的中间位置而不是右下角的问题。\n显示归档页面副标题 添加\u0026quot;article-subtitle\u0026quot;样式（assets/scss/partials/article.scss）\n1 2 3 4 5 6 7 8 .article-subtitle { margin-top: -5px; font-size: 1.5rem; @include respond(md) { font-size: 1.6rem; } } 再修改\u0026quot;article-details\u0026quot;，加上\u0026quot;article-subtitle\u0026quot;（layouts/partials/article-list/compact.html）\n1 2 3 4 5 6 7 8 9 \u0026lt;div class=\u0026#34;article-details\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;article-title\u0026#34;\u0026gt; {{- .Title -}} \u0026lt;/h2\u0026gt; {{ with .Params.description }} \u0026lt;div class=\u0026#34;article-subtitle\u0026#34;\u0026gt; {{ . }} \u0026lt;/div\u0026gt; {{ end }} 字数统计 在页面下方显示文章数目和总体字数（layouts/partials/footer/footer.html）\n1 2 3 4 5 6 7 8 9 \u0026lt;!-- Add total page and word count time --\u0026gt; \u0026lt;section class=\u0026#34;totalcount\u0026#34;\u0026gt; {{$scratch := newScratch}} {{ range (where .Site.Pages \u0026#34;Kind\u0026#34; \u0026#34;page\u0026#34; )}} {{$scratch.Add \u0026#34;total\u0026#34; .WordCount}} {{ end }} 发表了{{ len (where .Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;post\u0026#34;) }}篇文章 · 总计{{ div ($scratch.Get \u0026#34;total\u0026#34;) 1000.0 | lang.FormatNumber 2 }}k字 \u0026lt;/section\u0026gt; 修改风格(assets/scss/partials/footer.scss)\n1 2 3 4 5 .totalcount { color: var(--card-text-color-secondary); font-weight: normal; margin-bottom: 5px; } 正文内容显示修改 图片圆角阴影和代码块样式(assets/scss/custom.scss)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 .article-page .main-article .article-content { img { max-width: 96% !important; height: auto !important; box-shadow: 0 0px 14px 2px #aeaeae85; border-radius: 8px; } } [data-scheme=light] .article-content .highlight { background-color: #ecf2ff; } [data-scheme=light] .chroma { color: #a076f9; background-color: #ecf2ffcc; } 添加网站追踪 原来的博客有用过Google analysis，这次换了umami，注册使用很简单。\n添加评论 使用waline系统。根据指导，先创建一个LeanCloud国际版作为数据库，然后使用Vercel部署服务端。完成之后进入settings-\u0026gt;Domains，vercel.app结尾的地址即为serverURL。注册好管理员就可以查看评论。\n配置到博客（config.yaml）\n1 2 3 4 5 6 7 8 9 10 11 12 13 waline: serverURL: 填入serverURL lang: zh-cn pageview: emoji: - https://unpkg.com/@waline/emojis@1.0.1/weibo requiredMeta: - name - email - url locale: admin: Admin placeholder: 添加聊天气泡 使用的是日本的channel.io。注册完进入设置，General-\u0026gt;Manage Plug-in-\u0026gt;Install plug-in for web-\u0026gt;Javascript，复制代码到layouts/partials/footer/custom.html完成。\n参考链接 https://gohugo.io/getting-started/usage/ https://stack.jimmycai.com/guide/getting-started https://ponder.lol/2023/custom-hugo-theme-stack/ https://blog.linsnow.cn/p/join-hugo-and-stack/ ","date":"2021-03-13T22:52:50+08:00","image":"https://kkpetra.github.io/p/hugo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/hugo_hu40cbd56aa319431e2f94c340d268efa8_55522_120x120_fill_box_smart1_3.png","permalink":"https://kkpetra.github.io/p/hugo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AE%B0%E5%BD%95/","title":"Hugo博客搭建记录"}]